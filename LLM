# Install and load required libraries
install.packages(c("tm", "wordcloud", "tidytext", "ggplot2", "dplyr", "topicmodels"))
library(tm)
library(wordcloud)
library(tidytext)
library(ggplot2)
library(dplyr)
library(topicmodels)

# 1. Load and Clean the Text Data
# Load data (replace 'your_file.csv' with your actual file path)
data <- read.csv("your_file.csv")

# Extract the column with text data (replace 'complaint_text' with your actual column name)
complaints <- data$complaint_text

# Create a Corpus
complaint_corpus <- Corpus(VectorSource(complaints))

# Clean the text: Convert to lower case, remove punctuation, stop words, numbers, and white spaces
complaint_corpus <- complaint_corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(stripWhitespace)

# 2. Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(complaint_corpus)

# Convert to a matrix and calculate word frequencies
word_freq <- as.matrix(dtm)
word_freq <- sort(colSums(word_freq), decreasing = TRUE)

# Convert to a data frame for visualization
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq)

# View the top 10 most frequent words
print(head(word_freq_df, 10))

# 3. Visualize Word Frequency with a Word Cloud
set.seed(123) # Ensure reproducibility
wordcloud(words = word_freq_df$word, freq = word_freq_df$freq, min.freq = 2,
          max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))

# 4. Sentiment Analysis
# Tokenize the text data
complaint_tokens <- data %>%
  unnest_tokens(word, complaint_text)

# Perform sentiment analysis using the Bing lexicon
sentiment_bing <- complaint_tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment)

# Plot sentiment
ggplot(sentiment_bing, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiment Analysis of Complaints",
       x = "Sentiment",
       y = "Count") +
  theme_minimal()

# 5. Bigram (Phrase) Analysis
# Tokenize into bigrams
bigrams <- data %>%
  unnest_tokens(bigram, complaint_text, token = "ngrams", n = 2)

# Count the most frequent bigrams
bigram_count <- bigrams %>%
  count(bigram, sort = TRUE)

# View the top 10 bigrams
print(head(bigram_count, 10))

# Visualize bigrams
bigram_count %>%
  filter(n > 10) %>%
  ggplot(aes(x = reorder(bigram, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Most Common Bigrams in Complaints",
       x = "Bigram",
       y = "Frequency")

# 6. Topic Modeling (Optional)
# Apply LDA with 5 topics
lda_model <- LDA(dtm, k = 5, control = list(seed = 123))

# View top terms for each topic
lda_terms <- terms(lda_model, 10)
print(lda_terms)
